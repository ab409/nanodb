CS122 Assignment 3 - Table Statistics and Plan Costing - Design Document
========================================================================

A:  Logistics
-------------

A1.  List your team name and the people who worked on this assignment.

     BBB

     Ryan Batterman
     Alex Bello
     Ronnel Boettcher

A2.  Specify the tag name and commit-hash of the Git version you are
     submitting for your assignment.  (You can list the commit hashes
     of your repository tags with this command:  git show-ref --tags)

     Tag name:     <tag>
     Commit hash:  <hash>

A3.  Specify any late tokens you are applying to this assignment, or
     "none" if no late tokens.

     None

A4.  Briefly describe what parts of the assignment each teammate focused on.

     Ryan focused on plan costing.
     Bello focused on selectivity estimations.
     Ronnel focused on statistics collection.

B:  Statistics Collection
-------------------------

B1.  Using pseudocode, summarize the implementation of your HeapTupleFile
     analyze() function.

     Initialize column stats collectors for each column in table.
     dbPage = first database page in DBFile for this HeapTupleFile
     numberOfPages = 0
     numberOfSlots = 0
     numberOfTuples = 0
     sumOfTupleSizes
     while dbPage is not null:
         numberOfPages = numberOfPages + 1
         numberOfSlots = numberOfSlots + number of slots in dbPage
         for slot in dbPage:
             ptup = tuple from slot
             numberOfTuples = numberOfTuples + 1
             sumOfTupleSizes = sumOfTupleSizes + ptup's size
         dbPage = next page
     Generate column stats for each column statistic collector
     Calculate average tuple size
     Initialize TableStats object with variables computed above, and the
        column stats.
     Save the stats

C:  Plan Costing Implementation
-------------------------------

C1.  Briefly describe how you estimate the number of tuples and the cost
     of a file-scan plan node.  What factors does your cost include?

     * The number of tuples is estimated using previous counts of the number of tuples.
     * The number of block IOs is equal to the number of data pages
     * The tuple size is the observed average tuple size from the last time analyze was called.
     * The CPU cost is the number of tuples.

C2.  Same question as for C1, but for simple filter nodes.
     * The number of tuples is estimated as the number of tuples of the child times the selectivity,
       where the selectivity is estimated assuming a uniform distribution of the values in the
       predicate column (essentially by using the computeRatio helper, as described in class)
     * The number of block IOs is the number of block IOs of the child.
     * The tuple size is the tuple size of the child.
     * The CPU cost is the CPU cost of the child + the number of tuples of the child.

C3.  Same question as for C1, but for nested-loop joins.
     * The number of tuples is:
       - selectivity(predicate) * (n_r + n_l) + delta for:
       	 * n_r, n_l := number of tuples in the right and left tables, respectively
	 * delta := n_r for left outer join, n_l for right outer, n_r + n_l for full outer
       - n_l for semijoin and antijoin
     * The number of block IOs is estimated as the sum of the number of block IOs of the left
       right children.
     * The tuple size is the sum of the tuple sizes of the left and right children.
     * The CPU cost is the sum of the CPU costs of the left and right children + n_r * n_l.

D:  Costing SQL Queries
-----------------------

Answer these questions after you have loaded the stores-28K.sql data, and
have analyzed all of the tables in that schema.

D1.  Paste the output of running:  EXPLAIN SELECT * FROM cities;
     Do not include debug lines, just the output of the command itself.
Explain Plan:
    FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=47.6, CPUCost=254.0, blockIOs=1]

Estimated 254.000000 tuples with average size 47.574802
Estimated number of block IOs:  1


D2.  What is the estimated number of tuples that will be produced by each
     of these queries:

     SELECT * FROM cities WHERE population > 1000000;

     225.582245

     SELECT * FROM cities WHERE population > 5000000;

     99.262199

     SELECT * FROM cities WHERE population > 8000000;

     4.522162

     How many tuples does each query produce?

     9, 1, 1 respectively

     Briefly explain the difference between the estimated number of tuples
     and the actual number of tuples for these queries.
     
     Population is not uniformly distributed; it follows a power law. This means
     we have way fewer large city than would be expected on the basis of a
     uniform distribution.

D3.  Paste the output of running these commands:

     EXPLAIN SELECT store_id FROM stores, cities
     WHERE stores.city_id = cities.city_id AND
           cities.population > 1000000;

Explain Plan:
    Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.2, tupSize=36.8, CPUCost=512030.3, blockIOs=5]
        NestedLoops[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.POPULATION > 1000000] cost=[tuples=1776.2, tupSize=36.8, CPUCost=510254.0, blockIOs=5]
            Rename[resultTableName=STORES] cost=[tuples=2000.0, tupSize=13.0, CPUCost=2000.0, blockIOs=4]
                FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, CPUCost=2000.0, blockIOs=4]
            Rename[resultTableName=CITIES] cost=[tuples=254.0, tupSize=23.8, CPUCost=254.0, blockIOs=1]
                FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, CPUCost=254.0, blockIOs=1]

Estimated 1776.238159 tuples with average size 36.787399
Estimated number of block IOs:  5

     EXPLAIN SELECT store_id FROM stores JOIN
                    (SELECT city_id FROM cities
                     WHERE population > 1000000) AS big_cities
                    ON stores.city_id = big_cities.city_id;

Explain Plan:
    Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.2, tupSize=36.8, CPUCost=455674.3, blockIOs=5]
        NestedLoops[pred:  STORES.CITY_ID == BIG_CITIES.CITY_ID] cost=[tuples=1776.2, tupSize=36.8, CPUCost=453898.1, blockIOs=5]
            Rename[resultTableName=STORES] cost=[tuples=2000.0, tupSize=13.0, CPUCost=2000.0, blockIOs=4]
                FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, CPUCost=2000.0, blockIOs=4]
            Rename[resultTableName=BIG_CITIES] cost=[tuples=225.6, tupSize=23.8, CPUCost=733.6, blockIOs=1]
                Project[values:  [CITIES.CITY_ID]] cost=[tuples=225.6, tupSize=23.8, CPUCost=733.6, blockIOs=1]
                    SimpleFilter[pred:  CITIES.POPULATION > 1000000] cost=[tuples=225.6, tupSize=23.8, CPUCost=508.0, blockIOs=1]
                        FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, CPUCost=254.0, blockIOs=1]

Estimated 1776.238159 tuples with average size 36.787399
Estimated number of block IOs:  5


     The estimated number of tuples produced should be the same, but the
     costs should be different.  Explain why.

     * The (expected) number of tuples produced is the same because the queries produce the same result.
     * The (expected) CPU costs are different because the former query iterates through n_r * n_l tuple pairs,
       checking each to see if it matches the conjunct; the latter cuts down on the size of the inner
       table first, then does a nested loop join. Different approaches --> different CPU costs!

D4.  The assignment gives this example "slow" query:

     SELECT store_id, property_costs
     FROM stores, cities, states
     WHERE stores.city_id = cities.city_id AND
           cities.state_id = states.state_id AND
           state_name = 'Oregon' AND property_costs > 500000;

     How long does this query take to run, in seconds?
     ~ 17.2s

     Include the EXPLAIN output for the above query here.

    Explain Plan:
        Project[values:  [STORES.STORE_ID, STORES.PROPERTY_COSTS]] cost=[tuples=62.4, tupSize=52.5, cpuCost=6987367.5, blockIOs=6]
            NestedLoops[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.STATE_ID == STATES.STATE_ID AND STATES.STATE_NAME == 'Oregon' AND STORES.PROPERTY_COSTS > 500000] cost=[tuples=62.4, tupSize=52.5, cpuCost=6987305.0, blockIOs=6]
                NestedLoops[no pred] cost=[tuples=127000.0, tupSize=36.8, cpuCost=510254.0, blockIOs=5]
                    Rename[resultTableName=STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                        FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                    Rename[resultTableName=CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                        FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                Rename[resultTableName=STATES] cost=[tuples=51.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]
                    FileScan[table:  STATES] cost=[tuples=51.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]

    Estimated 62.437443 tuples with average size 52.454067
    Estimated number of block IOs:  6


     How would you rewrite this query (e.g. using ON clauses, subqueries
     in the FROM clause, etc.) to be as optimal as possible?  Also include
     the result of EXPLAINing your query.
     
     The following should be significantly faster:
        SELECT store_id, property_costs FROM (stores INNER JOIN cities USING (city_id) WHERE property_costs > 500000) a INNER JOIN states USING (state_id) WHERE state_name = 'Oregon';

    HOWEVER, it won't give the correct result due to some planner bug from last week :(((.

    Explain result: [[[TODO]]]



E:  Extra Credit [OPTIONAL]
---------------------------

If you implemented any extra-credit tasks for this assignment, describe
them here.  The description should be like this, with stuff in "<>" replaced.
(The value i starts at 1 and increments...)

E<i>:  <one-line description>

     <brief summary of what you did, including the specific classes that
     we should look at for your implementation>

     <brief summary of test-cases that demonstrate/exercise your extra work>

F:  Feedback [OPTIONAL]
-----------------------

These questions are optional, and they obviously won't affect your grade
in any way (including if you hate everything about the assignment and
databases in general, or Donnie and the TAs in particular).

NOTE:  If you wish to give anonymous feedback, a similar survey will be
       made available on the Moodle.  

F1.  How many hours total did your team spend on this assignment?
     (That is, the sum of each teammate's time spent on the assignment.)

F2.  What parts of the assignment were most time-consuming?

F3.  Which parts of the assignment did you particularly enjoy?

F4.  Which parts did you particularly dislike?

F5.  Do you have any suggestions for how future versions of the
     assignment can be improved?

